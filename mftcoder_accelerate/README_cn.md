# MFTCoder: Accelerate + DeepSpeedæ¡†æ¶ç¯‡
[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/codefuse-ai)
<a href="https://github.com/codefuse-ai/MFTCoder/blob/main/LICENSE">
    <img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue">
</a>

[**ä¸­æ–‡**] [[English]](README.md)

## 1. æ›´æ–°

ğŸ”¥ MFTCoderåœ¨Huggingface accelerate + DeepSpeedæ¡†æ¶ä¸‹æ”¯æŒQLoRA/LoRAå¾®è°ƒï¼› 

ğŸ”¥ MFTCoderåœ¨è®­ç»ƒä¸­æ”¯æŒäº†å¤šä»»åŠ¡å¾®è°ƒï¼Œ å¯ä»¥åŒæ—¶å¹³è¡¡å¤šä¸ªä»»åŠ¡çš„è®­ç»ƒï¼Œè®­ç»ƒçš„æ¨¡å‹æ”¯æŒå¤šä»»åŠ¡æ¨ç†ï¼› 

ğŸ”¥ MFTCoderåœ¨è®­ç»ƒä¸­æ”¯æŒå¤šç§æ¨¡å‹åŸºåº§ï¼š codellama, llama2, llama, starcoder, codegeex2, chatglm2, qwenç­‰

## 2. æ•°æ®æ ¼å¼
### 2.1 è®­ç»ƒæ•°æ®æ ¼å¼
è®­ç»ƒæ•°æ®ä¸ºjsonlæ ¼å¼ï¼Œæ¯ä¸€è¡Œçš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼Œå…¶ä¸­chat_roundså­—æ®µæ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚æ·»åŠ æˆ–åˆ é™¤å…¶ä»–å­—æ®µã€‚
å¯ä»¥å‚è€ƒé¡¹ç›®ä¸­çš„xxx.jsonlæ–‡ä»¶ã€‚
```json
{
    "id":0,
    "data_name":"code-helper",
    "chat_rounds":[
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»£ç åŠ©æ‰‹ï¼Œå¯ä»¥å›å¤ç”¨æˆ·ä¸ä»£ç ç›¸å…³çš„é—®é¢˜",
            "chat_round_id": 0
        },
        {
            "role": "human",
            "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åº", 
            "chat_round_id": 1
        },
        {
            "role": "bot",
            "content": "ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•xxxxxx", 
            "chat_round_id": 1
        },
        {
            "role": "human",
            "content": "è§£é‡Šä¸€ä¸‹è¿™æ®µä»£ç ", 
            "chat_round_id": 2
        },
        {
            "role": "bot",
            "content": "å¥½çš„ï¼Œè¿™æ®µä»£ç xxx", 
            "chat_round_id": 2
        }
    ]
}
```

### 2.2 æ¨ç†æ•°æ®æ ¼å¼
æ¨ç†æ•°æ®æ ¼å¼ä¸ºæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®æ ¼å¼ä¸‹æ‹¼æ¥çš„å­—ç¬¦ä¸²å½¢å¼ï¼Œå®ƒä¹Ÿæ˜¯æ¨ç†æ—¶è¾“å…¥promptæ‹¼æ¥çš„æ–¹å¼ï¼š
```python
"""
<s>system
è¿™æ˜¯SystemæŒ‡ä»¤
<s>human
è¿™æ˜¯ç¬¬1è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<s>bot
è¿™æ˜¯ç¬¬1è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹{EOS_TOKEN}
<s>human
è¿™æ˜¯ç¬¬2è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<s>bot
è¿™æ˜¯ç¬¬2è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹{EOS_TOKEN}
...
...
...
<s>human
è¿™æ˜¯ç¬¬nè½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<s>bot
{æ¨¡å‹ç°åœ¨è¦ç”Ÿæˆçš„å†…å®¹}{EOS_TOKEN}
"""
```


## 3. æ¨¡å‹è®­ç»ƒ
ç›®å‰æ”¯æŒå…¨é‡å‚æ•°æŒ‡ä»¤å¾®è°ƒã€QLoRAæŒ‡ä»¤å¾®è°ƒï¼ŒLoRAæŒ‡ä»¤å¾®è°ƒã€‚
ä¸€äº›ä¼˜ç§€çš„ä»£ç é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œç†è®ºä¸Šï¼ŒHuggingFaceä¸Šå¼€æºçš„æ¨¡å‹ï¼Œå‡å¯ä½¿ç”¨æœ¬é¡¹ç›®è¿›è¡Œè®­ç»ƒï¼š

ğŸ¤— [æœ€æ–°ä»£ç é¢„è®­ç»ƒSOTAï¼ŒCodeLlama](https://huggingface.co/codellama/CodeLlama-34b-Python-hf) ï¼šcode-llama-34bï¼Œ code-llama-34b-python, æ–°çš„SOTAåŸºåº§ã€‚

ğŸ¤— [10Bçº§åˆ«æœ€ä½³ä»£ç é¢„è®­ç»ƒæ¨¡å‹Starcoder](https://huggingface.co/bigcode/starcoder) wizardCoder-15B, PanGu-coder2ç­‰å‰SOTAçš„åŸºåº§æ¨¡å‹ã€‚

ğŸ¤— [å¤šè¯­è¨€èƒ½æ‰‹Qwen-7b](https://huggingface.co/Qwen/Qwen-7B) ï¼šé€‚ç”¨äºå¤šè¯­è¨€ä»»åŠ¡ï¼Œä¹Ÿé€‚ç”¨ä¸­æ–‡ä»»åŠ¡ã€‚è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ—¶ã€‚

æˆ‘ä»¬å°†è®­ç»ƒä¸­ä½¿ç”¨çš„å„ç§ç»„ä»¶æŠ½å–å‡ºæ¥ï¼Œä»¥ä¾¿åç»­çš„æ‰©å±•å’Œä¼˜åŒ–ï¼Œè¯¦è§srcç›®å½•ä¸‹çš„å®ç°ã€‚
å¾®è°ƒè®­ç»ƒçš„æ ¹ç›®å½•æ˜¯```mftcoder_accelerate/src/```, 

è®­ç»ƒå…¥å£æ–‡ä»¶æ˜¯```mftcoder_accelerate/src/pefts/mft_accelerate.py```

å‚æ•°é…ç½®å­˜å‚¨åœ¨```mftcoder_accelerate/src/configs```ç›®å½•ä¸‹ï¼Œæ–¹ä¾¿ç»Ÿä¸€ç®¡ç†å’Œæ›´æ”¹ã€‚

**_æ‰€ä»¥ï¼Œåœ¨ä½ å¼€å¯è®­ç»ƒä¹‹å‰ï¼Œè¯·è¿›å…¥srcç›®å½•_**
```
cd mftcoder_accelerate/src
```

### 3.1 æ•°æ®tokenization
è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬å°†å¤šè½®å¯¹è¯æ‹¼æ¥æˆå¦‚ä¸‹æ ¼å¼ï¼ˆä¹Ÿæ˜¯ä¸Šæ–‡ä¸­çš„æ¨ç†stringæ ¼å¼ï¼‰ï¼Œç„¶åè¿›è¡Œtokenizeã€‚å…¶ä¸­```<s>human\n```è¡¨ç¤ºhumanè¾“å…¥æç¤ºç¬¦ï¼Œ```<s>bot\n```è¡¨ç¤ºbotè¾“å‡ºæç¤ºç¬¦ï¼Œ```{EOS_TOKEN}``` è¡¨ç¤ºeos_tokenã€‚
å…¶ä¸­eos_tokenå¯ä»¥æ ¹æ®ä¸åŒæ¨¡å‹ä¿®æ”¹æ›¿æ¢ã€‚
```
"<s>human\n{input1}<s>bot\n{target1}{EOS_TOKEN}<s>human\n{input2}<s>bot\n{target2}{EOS_TOKEN}\n"
```
åœ¨è®¡ç®—lossæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡loss maskçš„æ–¹å¼ï¼Œinputéƒ¨åˆ†çš„lossä¸å‚ä¸å‚æ•°æ›´æ–°ï¼Œåªæœ‰â€œtarget{EOS_TOKEN}â€éƒ¨åˆ†çš„losså‚ä¸å‚æ•°æ›´æ–°ã€‚
è¿™ç§æ–¹å¼å……åˆ†åˆ©ç”¨äº†æ¨¡å‹å¹¶è¡Œè®¡ç®—çš„ä¼˜åŠ¿ï¼Œè®­ç»ƒæ›´åŠ é«˜æ•ˆï¼ŒåŒæ—¶ä¹Ÿå……åˆ†åˆ©ç”¨äº†decoder-onlyæ¨¡å‹ä»å·¦åˆ°å³attentionçš„ç‰¹æ€§ï¼Œä¸€æ¬¡æ€§å°†å¤šè½®å¯¹è¯ä¸­çš„æ¯ä¸ªtargetéƒ¨åˆ†éƒ½å‚ä¸äº†è®­ç»ƒï¼Œè®­ç»ƒæ›´å……åˆ†é«˜æ•ˆã€‚

### 3.2 LoRA/QLoRAå¾®è°ƒ
å…³äºLoRAçš„è¯¦ç»†ä»‹ç»å¯å‚è€ƒè®ºæ–‡ï¼š[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)

å…³äºQLoRAçš„è¯¦ç»†ä»‹ç»å¯å‚è€ƒè®ºæ–‡ï¼š[QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)

QLoRAé€šè¿‡4-bitçš„nf4é‡åŒ–ï¼Œä¸”åŠ å…¥æ›´å¤šadapterï¼Œåœ¨å¤§å¹…å‡å°‘æ˜¾å­˜æ¶ˆè€—çš„åŒæ—¶ï¼Œå°½å¯èƒ½é€¼è¿‘å…¨é‡å‚æ•°å¾®è°ƒçš„æ•ˆæœã€‚
QLoRAè®ºæ–‡æŒ‡å‡ºï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸€å¼ V100ä¸Šå¯¹33Bçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”æ€§èƒ½é€¼è¿‘å…¨é‡å‚æ•°å¾®è°ƒã€‚

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯è¿›è¡ŒLora/QLora/å…¨é‡ å¾®è°ƒï¼š

deepspeedé…ç½®åœ¨accelerate_ds_config.yamlä¸­ã€‚
```bash
accelerate launch --config_file accelerate_ds_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json
```
æˆ–è€…

ä¿®æ”¹å¹¶æ‰§è¡Œå¦‚ä¸‹shè„šæœ¬ï¼š

deepspeedé…ç½®åœ¨è„šæœ¬ä¸­é€šè¿‡å‘½ä»¤è¡Œè¾“å…¥ã€‚
```bash
sh ds_single_launch.sh
```

_**è®­ç»ƒéœ€è¦çš„å‚æ•°é…ç½®åœ¨```configs/*_train_config```ä¸­ï¼Œä¸»è¦å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š**_

- load_raw_dataset : éœ€è¦ä¿æŒtrueï¼Œåç»­ä¼šæ”¯æŒå…¶å®ƒæ¨¡å¼æ•°æ®ï¼Œå½“å‰ä»…æ”¯æŒjsonlè¾“å…¥
- data_paths: "[path1,path2,path3]" è¾“å…¥æ•°æ®åœ°å€ï¼Œå­—ç¬¦ä¸²ï¼Œå¼€å¤´ç»“å°¾ç”¨[]ï¼Œä¸­é—´ç”¨```,```é—´éš”ä¸åŒpathï¼Œæ¯ä¸ªpathæ˜¯ä¸€ä¸ªç›®å½•ï¼Œç›®å½•çš„æœ€åä¸€çº§åå­—ä½œä¸ºä»»åŠ¡åç§°ï¼Œä¸‹é¢åŒ…å«1åˆ°å¤šä¸ªjsonlæ•°æ®
- output_dirï¼šè®­ç»ƒè¾“å‡ºç›®å½•ï¼Œå­˜å‚¨checkpointã€lora_adaptorç­‰
- tb_dir: å­˜å‚¨tensorboardç­‰
- model_type: "llama|starcoder|chatglm2|qwen|gpt_nex"
- attn_implementation: "flash_attention_2" æˆ–è€… "eager"
- peft_type: loraæˆ–è€…qlora
- lora_rank: lora rank
- lora_alpha: lora alpha
- lora_dropout: lora dropout
- target_modules: List[str], loraç›®æ ‡æ¨¡å—ï¼Œå¦‚æœnullï¼Œä¼šä½¿ç”¨é»˜è®¤ï¼Œå‚è€ƒmodel_mapping.py
- quantization: æ˜¯å¦é‡åŒ–ï¼Œ"4bit", "8bit" æˆ–è€…nullï¼Œ qloraæ¨è4bité‡åŒ–
- pretrained_model_pathï¼šé¢„è®­ç»ƒæ¨¡å‹çš„æœ¬åœ°ç›®å½•ï¼Œæˆ–è€…åœ¨huggingfaceä¸Šçš„æ¨¡å‹åç§°ã€‚
- weighted_loss_mode: å¤šä»»åŠ¡lossåŠ æƒæ¨¡å¼ï¼Œ "case3"æ˜¯å½“å‰æ¨èã€‚
- padding_mode: æ•°æ®çš„æ ·æœ¬ç»„ç»‡æ–¹å¼ï¼Œ "padding"æ˜¯å°†æ¯ä¸ªåŸå§‹æ ·æœ¬å¡«å……åˆ°seq_length, "pack"æ˜¯å°†å°½é‡å¤šçš„æ ·æœ¬æ‰“åŒ…åˆ°æ¯ä¸ªseq_lengthçš„åºåˆ—ä¸­ã€‚
- num_train_epochsï¼šè®­ç»ƒçš„è½®æ¬¡ã€‚å¦‚æœæ•°æ®é‡è¶³å¤Ÿå¤§ï¼Œä¸€èˆ¬å»ºè®®åªè®­1-2ä¸ªepochã€‚
- per_device_train_batch_sizeï¼šæ¯å¼ æ˜¾å¡trainçš„batch sizeã€‚
- per_device_eval_batch_sizeï¼šæ¯å¼ æ˜¾å¡evalçš„batch sizeã€‚
- gradient_accumulation_stepsï¼šæ¢¯åº¦ç´¯è®¡æ­¥æ•°ã€‚global batch=num_gpus * per_device_train_batch_size * gradient_accumulation_stepsã€‚
- learning_rateï¼šå­¦ä¹ ç‡ã€‚å…¨é‡å‚æ•°å¾®è°ƒçš„æ—¶å€™ï¼Œå»ºè®®å°ä¸€äº›ï¼Œ1e-5æˆ–5e-6ã€‚qloraä¸­çš„å­¦ä¹ ç‡è®¾ç½®æ›´å¤§ä¸€äº›ï¼Œä¸€èˆ¬ä¸º1e-4ã€2e-4ã€‚
- min_lr: æœ€ä½å­¦ä¹ ç‡ï¼Œ ä¸€èˆ¬æ˜¯learning_rateçš„ååˆ†ä¹‹ä¸€
- seq_lengthï¼šè®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦ã€‚æŒ‰ç…§è‡ªå·±çš„è®¾å¤‡è¿›è¡Œè®¾ç½®ï¼Œè¶Šé•¿éœ€è¦å ç”¨è¶Šå¤šæ˜¾å­˜ã€‚
- log_intervalï¼šæ¯éš”å¤šå°‘æ­¥ç»Ÿè®¡ä¸€æ¬¡train lossã€‚
- checkpointing_stepsï¼šæ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€ä¸ªæ¨¡å‹ã€‚
- evalation_stepsï¼šæ¯éš”å¤šå°‘æ­¥åœ¨éªŒè¯é›†ä¸Ševaluateä¸€æ¬¡ã€‚
- early_stopping ï¼š æ˜¯å¦æ‰§è¡Œearly_stop
- early_stopping_stall_numï¼š å¤šå°‘ä¸ªeval pointä¸ç»§ç»­æ”¶æ•›ï¼Œåˆ™åœæ­¢è®­ç»ƒ
- lr_scheduler_typeï¼šå­¦ä¹ ç‡å˜åŒ–ç­–ç•¥ã€‚å¸¸ç”¨"cosine"
- warmup_stepsï¼šwarm upæ­¥æ•°ã€‚å­¦ä¹ ç‡ç»è¿‡å¤šå°‘æ­¥ï¼Œå¢é•¿åˆ°æŒ‡å®šçš„æ•°å€¼ã€‚
- seedï¼šéšæœºç§å­ï¼Œç”¨äºå¤ç°å®éªŒç»“æœã€‚
- saving_limitï¼šæ•´æ•°ï¼Œckptå­˜å‚¨æ•°é‡ä¸Šé™ï¼Œ å…¨é‡è®­ç»ƒå¿…é¡»è®¾ç½®ã€‚é»˜è®¤nullå³ä¸é™åˆ¶æ•°é‡ã€‚

## 4. æ¨¡å‹ä½¿ç”¨

### 4.1 æƒé‡åˆå¹¶
å¦‚æœä½¿ç”¨LoRAæˆ–è€…QLoRAè¿›è¡Œè®­ç»ƒï¼Œæœ¬é¡¹ç›®ä»…ä¿å­˜adapterçš„æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œéœ€è¦å°†adapteræƒé‡ä¸base modelè¿›è¡Œåˆå¹¶ã€‚
å¯ä»¥ä½¿ç”¨å¦‚ä¸‹merge_base_and_lora_to_hf.pyè„šæœ¬ã€‚
```
python pefts/merge_base_and_lora_to_hf.py \
    --base_model_or_path model_path \
    --adaptor_path lora_adapter_path \
    --model_type model_type \
    --merged_output_path output_path
```

### 4.2 æ¨¡å‹æ¨ç†
æˆ‘ä»¬æä¾›äº†å•è½®å¯¹è¯å’Œå¤šè½®å¯¹è¯çš„å¦‚ä¸‹è„šæœ¬ï¼Œè¯¥è„šæœ¬å¯åŒæ—¶å…¼å®¹å¤§éƒ¨åˆ†huggingfaceæ ¼å¼çš„æ¨¡å‹ã€‚
```python
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
)
model_name_or_path = "codefuse-ai/CodeFuse-Deepseek-33B"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, padding_side="left")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("<ï½œendâ–ofâ–sentenceï½œ>")
tokenizer.pad_token_id = tokenizer.eos_token_id
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)

HUMAN_ROLE_START_TAG = "<s>human\n"
BOT_ROLE_START_TAG = "<s>bot\n"
texts = ["write a python function of quick sort."]
texts = [f"{HUMAN_ROLE_START_TAG}{text}{BOT_ROLE_START_TAG}" for text in texts]

inputs = tokenizer(texts, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
```


ç”Ÿæˆè„šæœ¬ä¸­çš„top_pã€temperatureã€repetition_penaltyã€do_sampleç­‰å‚æ•°å¯¹æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœå½±å“è¾ƒå¤§ï¼Œå¯æŒ‰ç…§è‡ªå·±çš„ä½¿ç”¨åœºæ™¯è¿›è¡Œè°ƒè¯•ä¿®æ”¹ã€‚
å®è·µä¸­ï¼Œåœ¨ä»£ç ç”Ÿæˆåœºæ™¯ä¸­ï¼Œå¦‚æœé‡‡æ ·æ¨¡å¼ï¼Œdo_sample=True, top_p=0.95, temperature=0.1æ˜¯pass@1æŒ‡æ ‡çš„ä¸é”™é€‰æ‹©ï¼›
å¦‚æœéé‡‡æ ·æ¨¡å¼ï¼Œ do_sample=False, beam_num=1æˆ–è€…3æ˜¯ä¸é”™çš„é€‰æ‹©ï¼Œå…¶ä¸­beam_num=1å³ä¸ºgreedy decodingã€‚

## 5. FAQ
#### é—®é¢˜1ï¼šOOMå¦‚ä½•è§£å†³ï¼Ÿ
å¦‚æœå‘ç”ŸOOMï¼Œå¯ä»¥ç¼©å°per_device_train_batch_sizeã€seq_lengthç­‰å‚æ•°æ¥ç¼“è§£ã€‚ç”±äºé¢å¯¹çš„æ¨¡å‹æ™®éè¾ƒå¤§ï¼ˆ6bï¼Œ 13bï¼Œ 34bï¼Œ 70bç­‰ï¼‰æˆ‘ä»¬å·²ç»é»˜è®¤ä½¿ç”¨gradient_checkpointingæŠ€æœ¯ï¼Œå¯ä»¥å¤§å¹…é™ä½æ˜¾å­˜å ç”¨ï¼Œä½†è®­ç»ƒé€Ÿåº¦ä¼šç¨æ…¢ä¸€äº›ã€‚

#### é—®é¢˜2ï¼šå®‰è£…åŒ…é”™è¯¯
å‚è€ƒinit_env.shå’Œrequirements.txt

#### é—®é¢˜3ï¼šå¦‚ä½•æŒ‡å®šä½¿ç”¨æŸäº›å¡è®­ç»ƒï¼Ÿ
é€šè¿‡å¦‚ä¸‹æ–¹å¼ï¼Œå³å¯æŒ‡å®šä½¿ç”¨0å’Œ1å·å¡è¿›è¡Œè®­ç»ƒ:
```bash
CUDA_VISIBLE_DEVICES=0,1 accelerate launch --config_file pefts/accelerate_ds_config.yaml mft_accelerate.py --train_config configs/xxx_train_config.json
```

#### é—®é¢˜4ï¼šå¦‚æœæ— æ³•å®‰è£…flash attention 2, è¯¥å¦‚ä½•è®­ç»ƒ
å‚æ•°"attn_implementation" è®¾ç½®æˆ "eager" å¯ä»¥ç”¨naive attention

å¦‚æœä½ å¯ä»¥è‡ªè¡Œå®‰è£…ç¯å¢ƒå¹¶ä½¿ç”¨torch>=2.1.1ï¼Œå¯ä»¥å°è¯•è®¾ç½®å‚æ•°"attn_implementation"ä¸º "sdpa"ã€‚è¿™æ ·ä¼šå°è¯•ä½¿ç”¨transformerså…¼å®¹çš„torch.nn.functional.scaled_dot_product_attentionã€‚æ”¯æŒçš„æ¨¡å‹ä¸å…¨é¢ã€‚

#### é—®é¢˜5ï¼šå½“å‰æ”¯æŒçš„æ¨¡å‹ä¸­ï¼Œæœ‰ä»€ä¹ˆåŒºåˆ«
å›½äº§å¤§æ¨¡å‹æ¯”å¦‚chatglm2ï¼Œ chatglm3ï¼Œ baichuan2ï¼Œ qwenï¼Œ aquila2ç­‰ï¼Œä½¿ç”¨çš„æ˜¯å’Œæ¨¡å‹å…±åŒå‘å¸ƒçš„modeling_xxx.py. 
å…¶å®ƒè¢«transformerså®˜æ–¹æ”¯æŒçš„å¤§æ¨¡å‹ï¼Œç”±äºå·²ç»å‡çº§æ”¯æŒflash attentionç­‰ï¼Œæ‰€ä»¥å…¨é¢åˆ‡æ¢åˆ°å®˜æ–¹çš„modelingæ”¯æŒè®­ç»ƒï¼Œä¹‹å‰çš„è‡ªå®šä¹‰modelingä¼šè¢«deprecated





