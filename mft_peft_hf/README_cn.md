# MFTCoderè®­ç»ƒ: Huggingface accelerate + DeepSpeedæ¡†æ¶ç¯‡
[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/codefuse-ai)
<a href="https://github.com/codefuse-ai/MFTCoder/blob/main/LICENSE">
    <img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue">
</a>

[**ä¸­æ–‡**] [[English]](README.md)

## 1. æ›´æ–°

ğŸ”¥ MFTCoderåœ¨Huggingface accelerate + DeepSpeedæ¡†æ¶ä¸‹æ”¯æŒQLoRA/LoRAå¾®è°ƒï¼› 

ğŸ”¥ MFTCoderåœ¨è®­ç»ƒä¸­æ”¯æŒäº†å¤šä»»åŠ¡å¾®è°ƒï¼Œ å¯ä»¥åŒæ—¶å¹³è¡¡å¤šä¸ªä»»åŠ¡çš„è®­ç»ƒï¼Œè®­ç»ƒçš„æ¨¡å‹æ”¯æŒå¤šä»»åŠ¡æ¨ç†ï¼› 

ğŸ”¥ MFTCoderåœ¨è®­ç»ƒä¸­æ”¯æŒå¤šç§æ¨¡å‹åŸºåº§ï¼š codellama, llama2, llama, starcoder, codegeex2, chatglm2, qwenç­‰

## 2. æ•°æ®æ ¼å¼
### 2.1 è®­ç»ƒæ•°æ®æ ¼å¼
è®­ç»ƒæ•°æ®ä¸ºjsonlæ ¼å¼ï¼Œæ¯ä¸€è¡Œçš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼Œå…¶ä¸­chat_roundså­—æ®µæ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚æ·»åŠ æˆ–åˆ é™¤å…¶ä»–å­—æ®µã€‚
å¯ä»¥å‚è€ƒé¡¹ç›®ä¸­çš„xxx.jsonlæ–‡ä»¶ã€‚
```json
{
    "id":0,
    "data_name":"code-helper",
    "chat_rounds":[
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»£ç åŠ©æ‰‹ï¼Œå¯ä»¥å›å¤ç”¨æˆ·ä¸ä»£ç ç›¸å…³çš„é—®é¢˜",
            "chat_round_id": 0
        },
        {
            "role": "human",
            "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åº", 
            "chat_round_id": 1
        },
        {
            "role": "bot",
            "content": "ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•xxxxxx", 
            "chat_round_id": 1
        },
        {
            "role": "human",
            "content": "è§£é‡Šä¸€ä¸‹è¿™æ®µä»£ç ", 
            "chat_round_id": 2
        },
        {
            "role": "bot",
            "content": "å¥½çš„ï¼Œè¿™æ®µä»£ç xxx", 
            "chat_round_id": 2
        }
    ]
}
```

### 2.2 æ¨ç†æ•°æ®æ ¼å¼
æ¨ç†æ•°æ®æ ¼å¼ä¸ºæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®æ ¼å¼ä¸‹æ‹¼æ¥çš„å­—ç¬¦ä¸²å½¢å¼ï¼Œå®ƒä¹Ÿæ˜¯æ¨ç†æ—¶è¾“å…¥promptæ‹¼æ¥çš„æ–¹å¼ï¼š
```python
"""
<|role_start|>system<|role_end|>è¿™æ˜¯SystemæŒ‡ä»¤
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬1è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>è¿™æ˜¯ç¬¬1è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹</s>
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬2è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>è¿™æ˜¯ç¬¬2è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹</s>
...
...
...
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬nè½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>{æ¨¡å‹ç°åœ¨è¦ç”Ÿæˆçš„å†…å®¹}</s>
"""
```


## 3. æ¨¡å‹è®­ç»ƒ
ç›®å‰æ”¯æŒå…¨é‡å‚æ•°æŒ‡ä»¤å¾®è°ƒã€QLoRAæŒ‡ä»¤å¾®è°ƒï¼ŒLoRAæŒ‡ä»¤å¾®è°ƒã€‚
ä¸€äº›ä¼˜ç§€çš„ä»£ç é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œç†è®ºä¸Šï¼ŒHuggingFaceä¸Šå¼€æºçš„æ¨¡å‹ï¼Œå‡å¯ä½¿ç”¨æœ¬é¡¹ç›®è¿›è¡Œè®­ç»ƒï¼š

ğŸ¤— [æœ€æ–°ä»£ç é¢„è®­ç»ƒSOTAï¼ŒCodeLlama](https://huggingface.co/codellama/CodeLlama-34b-Python-hf) ï¼šcode-llama-34bï¼Œ code-llama-34b-python, æ–°çš„SOTAåŸºåº§ã€‚

ğŸ¤— [10Bçº§åˆ«æœ€ä½³ä»£ç é¢„è®­ç»ƒæ¨¡å‹Starcoder](https://huggingface.co/bigcode/starcoder) wizardCoder-15B, PanGu-coder2ç­‰å‰SOTAçš„åŸºåº§æ¨¡å‹ã€‚

ğŸ¤— [å¤šè¯­è¨€èƒ½æ‰‹Qwen-7b](https://huggingface.co/Qwen/Qwen-7B) ï¼šé€‚ç”¨äºå¤šè¯­è¨€ä»»åŠ¡ï¼Œä¹Ÿé€‚ç”¨ä¸­æ–‡ä»»åŠ¡ã€‚è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ—¶ã€‚

æˆ‘ä»¬å°†è®­ç»ƒä¸­ä½¿ç”¨çš„å„ç§ç»„ä»¶æŠ½å–å‡ºæ¥ï¼Œä»¥ä¾¿åç»­çš„æ‰©å±•å’Œä¼˜åŒ–ï¼Œè¯¦è§srcç›®å½•ä¸‹çš„å®ç°ã€‚å¾®è°ƒè®­ç»ƒçš„å…¥å£ç›®å½•æ˜¯```src/pefts```, è®­ç»ƒå…¥å£æ–‡ä»¶æ˜¯```src/pefts/mft_accelerate.py```, å‚æ•°é…ç½®å­˜å‚¨åœ¨```src/pefts/configs```ç›®å½•ä¸‹ï¼Œæ–¹ä¾¿ç»Ÿä¸€ç®¡ç†å’Œæ›´æ”¹ã€‚

### 3.1 æ•°æ®tokenization
è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬å°†å¤šè½®å¯¹è¯æ‹¼æ¥æˆå¦‚ä¸‹æ ¼å¼ï¼ˆä¹Ÿæ˜¯ä¸Šæ–‡ä¸­çš„æ¨ç†stringæ ¼å¼ï¼‰ï¼Œç„¶åè¿›è¡Œtokenizeã€‚å…¶ä¸­<|role_start|>human<|role_end|>è¡¨ç¤ºhumanè¾“å…¥æç¤ºç¬¦ï¼Œ<|role_start|>bot<|role_end|>è¡¨ç¤ºbotè¾“å‡ºæç¤ºç¬¦ï¼Œ`````</s>````` è¡¨ç¤ºeos_tokenã€‚
å…¶ä¸­eos_tokenå¯ä»¥æ ¹æ®ä¸åŒæ¨¡å‹ä¿®æ”¹æ›¿æ¢ã€‚
```
"<|role_start|>human<|role_end|>input1</s>target1</s>input2</s>target2</s>...
```
åœ¨è®¡ç®—lossæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡loss maskçš„æ–¹å¼ï¼Œinputéƒ¨åˆ†çš„lossä¸å‚ä¸å‚æ•°æ›´æ–°ï¼Œåªæœ‰â€œtarget</s>â€éƒ¨åˆ†çš„losså‚ä¸å‚æ•°æ›´æ–°ã€‚
è¿™ç§æ–¹å¼å……åˆ†åˆ©ç”¨äº†æ¨¡å‹å¹¶è¡Œè®¡ç®—çš„ä¼˜åŠ¿ï¼Œè®­ç»ƒæ›´åŠ é«˜æ•ˆï¼ŒåŒæ—¶ä¹Ÿå……åˆ†åˆ©ç”¨äº†decoder-onlyæ¨¡å‹ä»å·¦åˆ°å³attentionçš„ç‰¹æ€§ï¼Œä¸€æ¬¡æ€§å°†å¤šè½®å¯¹è¯ä¸­çš„æ¯ä¸ªtargetéƒ¨åˆ†éƒ½å‚ä¸äº†è®­ç»ƒï¼Œè®­ç»ƒæ›´å……åˆ†é«˜æ•ˆã€‚

### 3.2 LoRA/QLoRAå¾®è°ƒ
å…³äºLoRAçš„è¯¦ç»†ä»‹ç»å¯å‚è€ƒè®ºæ–‡ï¼š[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)
å…³äºQLoRAçš„è¯¦ç»†ä»‹ç»å¯å‚è€ƒè®ºæ–‡ï¼š[QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)

QLoRAé€šè¿‡4-bitçš„nf4é‡åŒ–ï¼Œä¸”åŠ å…¥æ›´å¤šadapterï¼Œåœ¨å¤§å¹…å‡å°‘æ˜¾å­˜æ¶ˆè€—çš„åŒæ—¶ï¼Œå°½å¯èƒ½é€¼è¿‘å…¨é‡å‚æ•°å¾®è°ƒçš„æ•ˆæœã€‚
QLoRAè®ºæ–‡æŒ‡å‡ºï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸€å¼ V100ä¸Šå¯¹33Bçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”æ€§èƒ½é€¼è¿‘å…¨é‡å‚æ•°å¾®è°ƒã€‚

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯è¿›è¡ŒLora/QLoraå¾®è°ƒï¼š
```bash
accelerate launch --config_file accelerate_ds_config.yaml mft_accelerate.py --train_config configs/starcoder_train_config.json
```

```configs/*_train_config```ä¸­çš„ä¸»è¦å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼Œä»¥ä¸‹å‚æ•°å¯ä»¥æ ¹æ®éœ€æ±‚è¿›è¡Œä¿®æ”¹ï¼Œå…¶ä»–å‚æ•°å»ºè®®ä¸åšä¿®æ”¹ï¼š
- load_raw_dataset : éœ€è¦ä¿æŒtrueï¼Œåç»­ä¼šæ”¯æŒå…¶å®ƒæ¨¡å¼æ•°æ®ï¼Œå½“å‰ä»…æ”¯æŒjsonlè¾“å…¥
- data_paths: "[path1,path2,path3]" è¾“å…¥æ•°æ®åœ°å€ï¼Œå­—ç¬¦ä¸²ï¼Œå¼€å¤´ç»“å°¾ç”¨[]ï¼Œä¸­é—´ç”¨```,```é—´éš”ä¸åŒpathï¼Œæ¯ä¸ªpathæ˜¯ä¸€ä¸ªç›®å½•ï¼Œç›®å½•çš„æœ€åä¸€çº§åå­—ä½œä¸ºä»»åŠ¡åç§°ï¼Œä¸‹é¢åŒ…å«1åˆ°å¤šä¸ªjsonlæ•°æ®
- output_dirï¼šè®­ç»ƒè¾“å‡ºç›®å½•ï¼Œå­˜å‚¨checkpointã€lora_adaptor checkpointç­‰
- tb_dir: å­˜å‚¨tensorboardç­‰
- model_type
- peft_type: loraæˆ–è€…qlora
- lora_rank: lora rank
- lora_alpha: lora alpha
- lora_dropout: lora dropout
- quantization: æ˜¯å¦é‡åŒ–ï¼Œ"4bit", "8bit" æˆ–è€…nullï¼Œ qloraæ¨è4bité‡åŒ–
- pretrained_model_pathï¼šé¢„è®­ç»ƒæ¨¡å‹çš„æœ¬åœ°ç›®å½•ï¼Œæˆ–è€…åœ¨huggingfaceä¸Šçš„æ¨¡å‹åç§°ã€‚
- **weighted_loss_mode**: å¤šä»»åŠ¡lossåŠ æƒæ¨¡å¼ï¼Œ "case3"æ˜¯å½“å‰æ¨èã€‚
- **padding_mode**: æ•°æ®çš„æ ·æœ¬ç»„ç»‡æ–¹å¼ï¼Œ "padding"æ˜¯å°†æ¯ä¸ªåŸå§‹æ ·æœ¬å¡«å……åˆ°seq_length, "pack"æ˜¯å°†å°½é‡å¤šçš„æ ·æœ¬æ‰“åŒ…åˆ°æ¯ä¸ªseq_lengthçš„åºåˆ—ä¸­ã€‚
- num_train_epochsï¼šè®­ç»ƒçš„è½®æ¬¡ã€‚å¦‚æœæ•°æ®é‡è¶³å¤Ÿå¤§ï¼Œä¸€èˆ¬å»ºè®®åªè®­1-2ä¸ªepochã€‚
- per_device_train_batch_sizeï¼šæ¯å¼ æ˜¾å¡trainçš„batch sizeã€‚
- per_device_eval_batch_sizeï¼šæ¯å¼ æ˜¾å¡evalçš„batch sizeã€‚
- gradient_accumulation_stepsï¼šæ¢¯åº¦ç´¯è®¡æ­¥æ•°ã€‚global batch=num_gpus * per_device_train_batch_size * gradient_accumulation_stepsã€‚
- learning_rateï¼šå­¦ä¹ ç‡ã€‚å…¨é‡å‚æ•°å¾®è°ƒçš„æ—¶å€™ï¼Œå»ºè®®å°ä¸€äº›ï¼Œ1e-5æˆ–5e-6ã€‚qloraä¸­çš„å­¦ä¹ ç‡è®¾ç½®æ›´å¤§ä¸€äº›ï¼Œä¸€èˆ¬ä¸º1e-4ã€2e-4ã€‚
- min_lr: æœ€ä½å­¦ä¹ ç‡ï¼Œ ä¸€èˆ¬æ˜¯learning_rateçš„ååˆ†ä¹‹ä¸€
- seq_lengthï¼šè®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦ã€‚æŒ‰ç…§è‡ªå·±çš„è®¾å¤‡è¿›è¡Œè®¾ç½®ï¼Œè¶Šé•¿éœ€è¦å ç”¨è¶Šå¤šæ˜¾å­˜ã€‚
- log_intervalï¼šæ¯éš”å¤šå°‘æ­¥ç»Ÿè®¡ä¸€æ¬¡train lossã€‚
- checkpointing_stepsï¼šæ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€ä¸ªæ¨¡å‹ã€‚
- evalation_stepsï¼šæ¯éš”å¤šå°‘æ­¥åœ¨éªŒè¯é›†ä¸Ševaluateä¸€æ¬¡ã€‚
- early_stopping ï¼š æ˜¯å¦æ‰§è¡Œearly_stop
- early_stopping_stall_numï¼š å¤šå°‘ä¸ªeval pointä¸ç»§ç»­æ”¶æ•›ï¼Œåˆ™åœæ­¢è®­ç»ƒ
- lr_scheduler_typeï¼šå­¦ä¹ ç‡å˜åŒ–ç­–ç•¥ã€‚
- warmup_stepsï¼šwarm upæ­¥æ•°ã€‚å­¦ä¹ ç‡ç»è¿‡å¤šå°‘æ­¥ï¼Œå¢é•¿åˆ°æŒ‡å®šçš„æ•°å€¼ã€‚
- seedï¼šéšæœºç§å­ï¼Œç”¨äºå¤ç°å®éªŒç»“æœã€‚

## 4. æ¨¡å‹ä½¿ç”¨

### 4.1 æƒé‡åˆå¹¶
å¦‚æœä½¿ç”¨LoRAæˆ–è€…QLoRAè¿›è¡Œè®­ç»ƒï¼Œæœ¬é¡¹ç›®ä»…ä¿å­˜adapterçš„æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œéœ€è¦å°†adapteræƒé‡ä¸base modelè¿›è¡Œåˆå¹¶ã€‚è„šæœ¬è§```src/pefts/merge_base_and_lora_to_hf.py```

### 4.2 æ¨¡å‹æ¨ç†
æˆ‘ä»¬æä¾›äº†å•è½®å¯¹è¯å’Œå¤šè½®å¯¹è¯çš„å¦‚ä¸‹è„šæœ¬ï¼Œè¯¥è„šæœ¬å¯åŒæ—¶å…¼å®¹å¤§éƒ¨åˆ†huggingfaceæ ¼å¼çš„æ¨¡å‹ã€‚
```python
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
)
tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)
tokenizer.padding_side = "left"
tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<unk>")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("</s>")
model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, trust_remote_code=True)

HUMAN_ROLE_START_TAG = "<|role_start|>human<|role_end|>"
BOT_ROLE_START_TAG = "<|role_start|>bot<|role_end|>"
texts = ["write a python function of quick sort."]
texts = [f"{HUMAN_ROLE_START_TAG}{text}{BOT_ROLE_START_TAG}" for text in texts]

inputs = tokenizer(texts, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
```


ç”Ÿæˆè„šæœ¬ä¸­çš„top_pã€temperatureã€repetition_penaltyã€do_sampleç­‰å‚æ•°å¯¹æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœå½±å“è¾ƒå¤§ï¼Œå¯æŒ‰ç…§è‡ªå·±çš„ä½¿ç”¨åœºæ™¯è¿›è¡Œè°ƒè¯•ä¿®æ”¹ã€‚
å®è·µä¸­ï¼Œåœ¨ä»£ç ç”Ÿæˆåœºæ™¯ä¸­ï¼Œå¦‚æœé‡‡æ ·æ¨¡å¼ï¼Œdo_sample=True, top_p=0.95, temperature=0.1æ˜¯pass@1æŒ‡æ ‡çš„ä¸é”™é€‰æ‹©ï¼›
å¦‚æœéé‡‡æ ·æ¨¡å¼ï¼Œ do_sample=False, beam_num=1æˆ–è€…3æ˜¯ä¸é”™çš„é€‰æ‹©ï¼Œå…¶ä¸­beam_num=1å³ä¸ºgreedy decodingã€‚

## 5. FAQ
#### é—®é¢˜1ï¼šOOMå¦‚ä½•è§£å†³ï¼Ÿ
å¦‚æœå‘ç”ŸOOMï¼Œå¯ä»¥ç¼©å°per_device_train_batch_sizeã€seq_lengthç­‰å‚æ•°æ¥ç¼“è§£ã€‚ç”±äºé¢å¯¹çš„æ¨¡å‹æ™®éè¾ƒå¤§ï¼ˆ6bï¼Œ 13bï¼Œ 34bï¼Œ 70bç­‰ï¼‰æˆ‘ä»¬å·²ç»é»˜è®¤ä½¿ç”¨gradient_checkpointingæŠ€æœ¯ï¼Œå¯ä»¥å¤§å¹…é™ä½æ˜¾å­˜å ç”¨ï¼Œä½†è®­ç»ƒé€Ÿåº¦ä¼šç¨æ…¢ä¸€äº›ã€‚

#### é—®é¢˜2ï¼šå®‰è£…åŒ…é”™è¯¯
å‚è€ƒinit_env.shå’Œrequirements.txt

#### é—®é¢˜3ï¼šå¦‚ä½•æŒ‡å®šä½¿ç”¨æŸäº›å¡è®­ç»ƒï¼Ÿ
é€šè¿‡å¦‚ä¸‹æ–¹å¼ï¼Œå³å¯æŒ‡å®šä½¿ç”¨0å’Œ1å·å¡è¿›è¡Œè®­ç»ƒ:
```bash
CUDA_VISIBLE_DEVICES=0,1 accelerate launch --config_file accelerate_ds_config.yaml mft_accelerate.py --train_config configs/starcoder_train_config.json
```





