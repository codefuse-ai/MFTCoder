# MFTCoder: Fine-Tuning & Inference & Evaluation & Submission
[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/codefuse-ai)
<a href="https://github.com/codefuse-ai/MFTCoder/blob/main/LICENSE">
    <img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue">
</a>

# SFT Fine-Tuning / SFTå¾®è°ƒ

To fine-tune the Qwen-1.8B model, you need to start by preparing the training dataset(s) and then proceed with the fine-tuning training using the dataset(s). Subsequently, we will outline the requirements for the training data format, provide instructions on building the configuration file, and guide you on initiating the training process.

å¾®è°ƒQwen-1.8Bæ¨¡å‹ï¼Œé¦–å…ˆéœ€è¦å‡†å¤‡è®­ç»ƒæ•°æ®é›†ä»¥ä¾¿éšåå°†å…¶ç”¨äºå¾®è°ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¯´æ˜è®­ç»ƒæ•°æ®æ ¼å¼è¦æ±‚ã€å¦‚ä½•é…ç½®æ¨¡å‹é…ç½®æ–‡ä»¶ï¼Œä»¥åŠå¦‚ä½•å¯åŠ¨å¾®è°ƒè®­ç»ƒè¿‡ç¨‹ã€‚

## Training Data Format / è®­ç»ƒæ•°æ®æ ¼å¼
The training data is in a uniformed JSONL format, in which each line of data has the following JSON format. The "chat_rounds" field is required, and other fields can be added or removed based on specific needs. In "chat_rounds", the element with "system" role is optional.

è®­ç»ƒæ•°æ®è¦æ±‚æ˜¯ç»Ÿä¸€çš„JSONLæ ¼å¼ï¼Œå³æ–‡ä»¶ï¼ˆæ‰©å±•åæ˜¯.jsonlï¼‰ä¸­æ¯è¡Œæ˜¯ä¸€é¡¹å¦‚ä¸‹JSONæ ¼å¼çš„æ•°æ®ã€‚æ ¼å¼ä¸­â€œchat_roundsâ€é¡¹æ˜¯å¿…é¡»çš„ï¼Œå…¶ä»–é¡¹å¯ä¾éœ€æ±‚å¢åˆ ã€‚åœ¨"chat_rounds"é¡¹ä¸­ï¼Œå¸¦æœ‰"system"è§’è‰²çš„é¡¹æ˜¯å¯é€‰çš„ã€‚

```json
{
    "id":0,
    "data_name":"code-helper",
    "chat_rounds":[
        {
            "role": "system",
            "content": "You are a expert in coding and help answer code questions",
            "chat_round_id": 0
        },
        {
            "role": "human",
            "content": "Write a python function of quick sort", 
            "chat_round_id": 1
        },
        {
            "role": "bot",
            "content": "Below is the function of quick sort: ...", 
            "chat_round_id": 1
        },
        {
            "role": "human",
            "content": "Explain the code", 
            "chat_round_id": 2
        },
        {
            "role": "bot",
            "content": "OK, this code ...", 
            "chat_round_id": 2
        }
    ]
}
```

An example is [CodeFuse-CodeExercise-Python-27k-dataset](https://modelscope.cn/datasets/codefuse-ai/CodeExercise-Python-27k/summary).

ä¸€ä¸ªå¯å‚è€ƒçš„ç¤ºä¾‹æ•°æ®é›†æ˜¯[CodeFuse-CodeExercise-Python-27k-dataset](https://modelscope.cn/datasets/codefuse-ai/CodeExercise-Python-27k/summary).

## Model Training / æ¨¡å‹è®­ç»ƒ

Currently, the "MFTCoder/mft_peft_hf" codebase supports [QLoRA](https://arxiv.org/pdf/2305.14314.pdf) instruction fine-tuning, and [LoRA](https://arxiv.org/pdf/2106.09685.pdf) instruction fine-tuning. According to the QLoRA paper, this method enables fine-tuning of a 33B model on a single V100 GPU while achieving performance close to that of full-parameter fine-tuning.
In theory, this project can be used to train any publicly available model in the HuggingFace Format.

å½“å‰ï¼Œ"MFTCoder/mft_peft_hf" ä»£ç æ”¯æŒ[QLoRA](https://arxiv.org/pdf/2305.14314.pdf)å’Œ[LoRA](https://arxiv.org/pdf/2106.09685.pdf)æŒ‡ä»¤å¾®è°ƒ. æŒ‰ç…§QLoRAè®ºæ–‡è¿™ç§æ–¹æ³•å¯åœ¨ä¸€å¼ V100æ˜¾å¡ä¸Šå¾®è°ƒå…·æœ‰33Bå‚æ•°çš„æ¨¡å‹ï¼Œå¹¶å–å¾—ä¸å…¨é‡å¾®è°ƒæƒ³æ¥è¿‘çš„ç»“æœã€‚ ç†è®ºä¸Šï¼Œè¯¥é¡¹ç›®å¯é€‚é…äºä»»ä½•å¯å…¬å¼€è·å¾—çš„HuggingFaceæ ¼å¼çš„æ¨¡å‹ã€‚

You can find the implementations in the ```mft_peft_hf/src``` directory. The entry directory for fine-tuning training is ```mft_peft_hf/src/pefts```, and the entry file for training is ```mft_peft_hf/src/pefts/mft_accelerate.py```. 
Configurations are stored in the ```mft_peft_hf/src/pefts/configs``` directory for easy management and modification.

å¯åœ¨```mft_peft_hf/src```ç›®å½•ä¸‹æ‰¾åˆ°å…·ä½“å®ç°ï¼Œå…¶ä¸­ï¼Œå¾®è°ƒè®­ç»ƒå…¥å£æ˜¯```mft_peft_hf/src/pefts```ï¼Œå…·ä½“å…¥å£æ–‡ä»¶æ˜¯```mft_peft_hf/src/pefts/mft_accelerate.py```ï¼Œè€Œå…·ä½“çš„é…ç½®æ–‡ä»¶ç»Ÿä¸€ä¿å­˜åœ¨```mft_peft_hf/src/pefts/configs```ç›®å½•ä¸‹ä»¥ä¾¿äºç®¡ç†ã€‚

More details you can find in this paper:

æ›´å¤šç»†èŠ‚å¯åœ¨ä¸‹é¢è¿™ç¯‡æ–‡ç« ä¸­å‘ç°ï¼š


```
@article{liu2023mftcoder,
  title={MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning},
  author={Liu, Bingchang and Chen, Chaoyu and Liao, Cong and Gong, Zi and Wang, Huan and Lei, Zhichao and Liang, Ming and Chen, Dajun and Shen, Min and Zhou, Hailian and others},
  journal={arXiv preprint arXiv:2311.02303},
  year={2023}
}
```

### Configuration / é…ç½®
An example configuration file for fine-tuning Qwen-1.8B model is [src/pefts/configs/qwen_train_config_1_8B.json](src/pefts/configs/qwen_train_config_1_8B.json).

ä¸€ä¸ªå…·ä½“çš„ç”¨äºå¾®è°ƒQwen-1.8Bæ¨¡å‹çš„é…ç½®æ–‡ä»¶æ˜¯ [src/pefts/configs/qwen_train_config_1_8B.json](src/pefts/configs/qwen_train_config_1_8B.json)ã€‚

The parameters in ```configs/*_train_config``` configuration files are explained as follows. **You can modify these parameters according to your needs**.

ä½äº```configs/*_train_config```ä¸‹çš„é…ç½®æ–‡ä»¶ä¸­çš„å„é¡¹å‚æ•°è§£é‡Šå¦‚ä¸‹ã€‚å¯æŒ‰è‡ªå·±çš„éœ€æ±‚è‡ªè¡Œè°ƒæ•´é…ç½®å‚æ•°å€¼ã€‚

- **load_raw_dataset**: Must be true at present. Only JSONL format is supported. *å¿…é¡»æ˜¯Trueï¼Œå¹¶ä¸”åªæ”¯æŒJSONLæ ¼å¼ã€‚*

- **data_paths**: Input data paths in a String of list format, e.g., "[path1,path2,path3]". Each path represents a task directory and each task directory contains one or more JSONL data files. You can provide one or more task directory. *å€¼å¿…é¡»æ˜¯å­—ç¬¦ä¸²æ ¼å¼çš„è·¯å¾„åˆ—è¡¨ï¼Œä¾‹å¦‚["è·¯å¾„1","è·¯å¾„2","è·¯å¾„3"]ã€‚æ¯ä¸ªè·¯å¾„ä»£è¡¨ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œä¸”å…¶ä¸­å¯åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªJSONLæ–‡ä»¶ã€‚è·¯å¾„æ•°é‡å¯ä»¥æ˜¯ä¸€ä¸ªæˆ–å¤šä¸ªã€‚*

- **output_dir**: Training output directory to store checkpoints, Lora adapter, etc. *ç”¨äºå­˜å‚¨è®­ç»ƒäº§ç”Ÿçš„æ£€æŸ¥ç‚¹çš„è¾“å‡ºç›®å½•è·¯å¾„*

- **tb_dir**: TensorBoard directory to store logs, metrics, etc. *ç”¨äºå­˜å‚¨è®­ç»ƒäº§ç”Ÿçš„TensorBoardæ—¥å­æ•°æ®çš„ç›®å½•ã€‚*

- **model_type**: Type of the model to train, e.g., "llama | starcoder | chatglm2 | qwen | gpt_neox". To fine-tune Qwen-1.8B, it must be "qwen". *è¦å¾®è°ƒçš„åº•åº§æ¨¡å‹ç±»å‹ï¼Œå¯é€‰å€¼å¦‚"llama | starcoder | chatglm2 | qwen | gpt_neox"ã€‚å¦‚æœè¦å¾®è°ƒQwen-1.8Bæ¨¡å‹ï¼Œåˆ™éœ€è®¾ä¸º"qwen"ã€‚*

- **peft_type**: either "lora" or "qlora". You can make a choice as your needs. *PEFTç±»å‹ï¼Œå¯é€‰å€¼åŒ…æ‹¬"lora"å’Œ"qlora"ï¼Œå¯è‡ªè¡Œé€‰æ‹©ã€‚*

- **lora_rank**: Rank value for Lora. *LoRA/QloRAæ¨¡å¼ä¸­çš„Rankå€¼ã€‚*

- **lora_alpha**: Alpha value for Lora. *LoRA/QLoRAæ¨¡å‹ä¸­çš„Alphaå€¼*

- **lora_dropout**: Dropout rate for Lora. *LoRA/QLoRAä¸­çš„dropoutç‡*

- **quantization**: Whether to use quantization."4bit" or "8bit", or null. For QLoRA, it is recommended to use 4-bit quantization. *å¯è®¾ç½®ä¸º"4bit"ã€"8bit"æˆ–è€…nullï¼Œnullæ„å‘³ç€ä¸é‡åŒ–ã€‚å¦‚æœæ˜¯ä½¿ç”¨QLoRAæ¨¡å‹ï¼Œæ¨èä½¿ç”¨"4bit"é‡åŒ–ã€‚*

- **pretrained_model_path**: Local/Shared disk path or model name on HuggingFace for the pre-trained model. *In Qwen AI competition, it should be the local path of your downloaded Qwen-1.8B model.* *è¦å¾®è°ƒçš„åº•åº§æ¨¡å‹çš„æœ¬åœ°æˆ–è¿œç¨‹è·¯å¾„ã€‚åœ¨Qwenæ¯”èµ›ä¸­ï¼Œåº”è¯¥å¡«å†™Qwen-1.8Bæ¨¡å‹çš„è·¯å¾„ã€‚*

- **weighted_loss_mode**: Loss weighting method for multitask training. "case3" is recommended at present. *å¤šä»»åŠ¡è®­ç»ƒæ¨¡å¼ä¸­çš„lossè®¡ç®—æ–¹æ³•ã€‚å½“å‰æ¨èä½¿ç”¨"case3"ç±»å‹ã€‚*

- **padding_mode**: The way tokenized data is set. "padding" means padding for each sample to seq_length, "pack" means putting samples into seq_length as many as possible. If you have large amounts of training samples, you may use "pack" to achieve faster training speed. *Tokenizationæ–¹å¼ï¼Œå¯é€‰å€¼æœ‰â€œpaddingâ€å’Œ"pack"ã€‚"padding"è¡¨ç¤ºbatchä¸­æ¯ä¸ªé¡¹ç›®ä¼šè¢«å¯¹é½åˆ°seq-lengthé•¿åº¦ï¼›"pack"è¡¨ç¤ºå°†å°½å¯èƒ½å¤šçš„æ ·æœ¬å¡«å……åˆ°ä¸€ä¸ªseq-lengthå¤§å°çš„æ–°æ ·æœ¬ä¸­ã€‚å¦‚æœä½ æœ‰å¾ˆå¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨"pack"æ–¹å¼è·å¾—æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚*

- **num_train_epochs**: Number of training epochs. *è®¡åˆ’è®­ç»ƒçš„Epochæ•°é‡*

- **per_device_train_batch_size**: Batch size per GPU for training. *è®­ç»ƒæ—¶å•å¡ä¸Šçš„batchå¤§å°*

- **per_device_eval_batch_size**: Batch size per GPU for evaluation. *éªŒè¯æ—¶å•å¡ä¸Šçš„batchå¤§å°*

- **gradient_accumulation_steps**: Number of gradient accumulation steps. Global batch size is calculated as num_gpus * per_device_train_batch_size * gradient_accumulation_steps. *æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚å…¨å±€batchå¤§å°ç­‰äºnum_gpusã€per_device_train_batch_sizeå’Œgradient_accumulation_stepsçš„ä¹˜ç§¯ã€‚*

- **learning_rate**: Initial Learning rate. For full-parameter fine-tuning, it is recommended to use a smaller value such as 1e-5 or 5e-6. For QLoRA, a larger learning rate is generally used, such as 1e-4 or 2e-4. *åˆå§‹å­¦ä¹ ç‡ã€‚å¯¹äºå…¨é‡å¾®è°ƒï¼Œæ¨èä½¿ç”¨å°ä¸€äº›çš„å€¼ï¼Œä¾‹å¦‚1e-5æˆ–1e-6ã€‚å¦‚æœæ˜¯QLoRAå¾®è°ƒï¼Œæ¨èä½¿ç”¨æ›´å¤§ä¸€äº›çš„å­¦ä¹ ç‡ï¼Œä¾‹å¦‚1e-4æˆ–2e-4ã€‚*

- **min_lr**: Minimum learning rate. Usually set to one-tenth of the learning rate. *æœ€å°å­¦ä¹ ç‡ï¼Œé€šå¸¸è®¾ç½®æ¯”åˆå§‹å­¦ä¹ ç‡å°ä¸€ä¸ªæ•°é‡çº§ã€‚*

- **seq_length**: Maximum input sequence length during training. *è®­ç»ƒè¿‡ç¨‹æœ€å¤§è¾“å…¥åºåˆ—é•¿åº¦ã€‚*

- **log_interval**: Log training loss every ```log_interval``` steps. *è®¾ç½®æ¯éš”å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—*

- **checkpointing_steps**: Save a checkpoint every ```checkpointing_steps``` steps. *è®¾ç½®æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹*

- **evaluation_steps**: Evaluate on the validation set every ```evaluation_steps``` steps. *è®¾ç½®æ¯éš”å¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡éªŒè¯*

- **early_stopping**: Enable early stopping or not. *è®¾ç½®æ˜¯å¦å¯ç”¨æ—©åœç­–ç•¥*

- **early_stopping_stall_num**: Number of evaluation points without improvement which triggers early stopping. *å¼€å¯æ—©åœç­–ç•¥æ—¶ï¼Œè®¾ç½®è¿ç»­å¤šå°‘ä¸ªéªŒè¯ç‚¹lossä¸ä¸‹é™ååœæ­¢è®­ç»ƒã€‚*

- **lr_scheduler_type**: Type of learning rate scheduler. "cosine" is a good choice already.  *å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼Œç›®å‰"cosine"æ˜¯ä¸é”™çš„é€‰æ‹©ã€‚*

- **num_warmup_steps**: Number of warm-up steps to gradually increase the learning rate. *è®¾ç½®é€æ­¥å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡æ‰€éœ€çš„æ­¥æ•°ã€‚*

- **seed**: Random seed for reproducibility. *è®¾ç½®éšæœºåŒ–ç§å­ï¼Œç”¨äºå¤ç°ä½¿ç”¨ã€‚*




### Run

To run LoRA/QLoRA fine-tuing, you can execute the [src/pefts/run_bash.sh](src/pefts/run_bash.sh) script:

è¦æ‰§è¡ŒLoRAæˆ–QLoRAå¾®è°ƒï¼Œå¯æ‰§è¡Œ[src/pefts/run_bash.sh](src/pefts/run_bash.sh)è„šæœ¬ï¼š

```shell
N_GPU_PER_NODE=8
N_NODE=1

accelerate launch \
  --num_machines $N_NODE \
  --num_processes $(($N_NODE*$N_GPU_PER_NODE)) \
  --use_deepspeed \
  --deepspeed_multinode_launcher 'standard' \
  --zero_stage 2 \
  --offload_optimizer_device 'cpu' \
  --offload_param_device 'none' \
  --gradient_accumulation_steps 1 \
  --gradient_clipping 1.0 \
  --zero3_init_flag false \
  --zero3_save_16bit_model false \
  --main_training_function 'main' \
  --mixed_precision 'bf16' \
  --dynamo_backend 'no' \
  --same_network \
  --machine_rank $RANK \
  --main_process_ip $MASTER_ADDR \
  --main_process_port $MASTER_PORT \
  --rdzv_backend 'static' \
  mft_accelerate.py --train_config configs/qwen_train_config_1_8B.json
```

You need to adjust some parametes as your needs, e.g. the configuration path ```--train_config```, the number of nodes ```N_NODE```, gpus of each node ```N_GPU_PER_NODE```. You can also execute the following command to run:

éœ€æŒ‰è‡ªå·±çš„æƒ…å†µè°ƒæ•´è„šæœ¬ä¸­çš„éƒ¨åˆ†å‚æ•°ï¼Œä¾‹å¦‚ï¼Œé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå³```--train_config```å‚æ•°çš„å€¼ï¼‰ã€æœºå™¨èŠ‚ç‚¹æ•°é‡```N_NODE```ã€æ¯ä¸ªèŠ‚ç‚¹GPUå¡æ•°```N_GPU_PER_NODE```ç­‰ã€‚ä¹Ÿå¯æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š


```bash
cd mft_peft_hf/src/pefts

accelerate launch --config_file accelerate_ds_config.yaml mft_accelerate.py --train_config configs/starcoder_train_config.json
```


# Inference Generation

We build our inference framework based on [bigcode-project/bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness). We recommend that you go to [bigcode-project/bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness) to learn some necessary information. We made some modifications to adapt to Qwen AI (Code) competition, including inference format, evaluation datasets localization et.al.

æ‰€ç”¨æ¨ç†æ¡†æ¶æ˜¯åŸºäº[bigcode-project/bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness)é¡¹ç›®æ„å»ºçš„ï¼Œå› æ­¤å»ºè®®è¿›å…¥è¯¥é¡¹ç›®ä¸»é¡µäº†è§£å¿…è¦çš„ä¿¡æ¯ã€‚åœ¨åŸé¡¹ç›®åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¿…è¦çš„è°ƒæ•´ä»¥é€‚é…Qwenèµ›äº‹ï¼Œä¾‹å¦‚æ¨ç†æ ¼å¼ã€è¯„æµ‹é›†æœ¬åœ°åŒ–ç­‰ã€‚

In the Qwen AI (Code) competition, we have chosen **HumanEvalPack** and **MBPP** as our evaluation tasks. Within the HumanEvalPack, we have selected two subtasks: "humanevalsynthesize" and "humanevalfixtests" each comprising 6 languages. In total, there are 2,468 questions to be evaluated. The complete list of evaluation tasks is as follows:

æœ¬æ¬¡Qwen AIï¼ˆä»£ç ï¼‰æ¯”èµ›ï¼Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨**HumanEvalPack** and **MBPP**ä½œä¸ºè¯„æµ‹ä»»åŠ¡ã€‚å…¶ä¸­ï¼ŒHumanEvalPackä¸­çš„ä¸¤ä¸ªå­ç±»"humanevalsynthesize" and "humanevalfixtests"è¢«å…·ä½“é€‰æ‹©ï¼Œè¿™ä¸¤ä¸ªå­ç±»æ¯ä¸ªéƒ½ç”±6ç§è¯­è¨€çš„è¡¥å…¨ä»»åŠ¡ç»„æˆã€‚æ€»ä½“ä¸Šï¼Œå…±æœ‰2468é“è¯„æµ‹é¢˜ç›®ã€‚å®Œæ•´çš„è¯„æµ‹ä»»åŠ¡åˆ—è¡¨å¦‚ä¸‹æ‰€ç¤ºï¼š


```
humanevalsynthesize-python
humanevalsynthesize-java
humanevalsynthesize-js
humanevalsynthesize-cpp
humanevalsynthesize-go
humanevalsynthesize-rust
humanevalfixtests-python
humanevalfixtests-java
humanevalfixtests-js
humanevalfixtests-cpp
humanevalfixtests-go
humanevalfixtests-rust
mbpp
```


## Inference/Tokenization Format / æ¨ç†æ ¼å¼

We take Qwen's ChatML format as our tokenization format:

æˆ‘ä»¬ä½¿ç”¨Qwenæ‰€ç”¨çš„ChatMLæ ¼å¼ä½œä¸ºæˆ‘ä»¬å¾®è°ƒè®­ç»ƒä¸­çš„tokenizationæ ¼å¼ï¼š

```
<|im_start|>system
{Here is your system prompt}<|im_end|>
<|im_start|>user
{Here is your 1st-round user prompt}<|im_end|>
<|im_start|>assistant
{Here is the model's inference result of 1st-round}<|im_end|>
<|im_start|>user
{Here is your 2nd-round user prompt}<|im_end|>
<|im_start|>assistant
{Here is the model's inference result of 2nd-round}<|im_end|>
...
```

The eod token is ```<|im_end|>```. During training, only the content of the "assistant" and the token "<|im_end|>" following it are taken into consideration for loss computation. You can find further details in [src/data/tokenization/preprocess_data.py](src/data/tokenization/preprocess_data.py). Based on this information, when attempting to generate inference results for evaluated tasks, please adhere to the reference format as follows:

EOD tokenæ˜¯```<|im_end|>```ã€‚è®­ç»ƒä¸­ï¼Œåªæœ‰"assistant"è§’è‰²çš„å†…å®¹å’Œå…¶åç´§è·Ÿçš„"<|im_end|>"tokenè¢«çº³å…¥lossè®¡ç®—ã€‚å¯åœ¨[src/data/tokenization/preprocess_data.py](src/data/tokenization/preprocess_data.py)å‘ç°æ›´å¤šå…³äºæ•°æ®å¤„ç†çš„ç»†èŠ‚ã€‚å½“ä½ è¯•å›¾ä¸ºè¯„æµ‹ä»»åŠ¡ç”Ÿæˆæ¨ç†ç»“æœæ—¶ï¼Œè¯·æŒ‰ç…§å¦‚ä¸‹çš„æ¨ç†æ ¼å¼ï¼š

```
<|im_start|>user
{Prompt of one evaluatation question}<|im_end|>
<|im_start|>assistant
```

Also, you can add a system prompt if you need as follows:

å¦å¤–ï¼Œä½ ä¹Ÿå¯æŒ‰è‡ªå·±çš„éœ€æ±‚å¢åŠ systemæç¤ºï¼š

```
<|im_start|>system
{This is your system prompt}<|im_end|>
<|im_start|>user
{Prompt of one evaluatation question}<|im_end|>
<|im_start|>assistant
```

## Inference Script / æ¨ç†è„šæœ¬

We have provided a shell script for inferring the evaluating tasks, i.e. [src/evaluation/launch_generate_codeqwen.sh](src/evaluation/launch_generate_codeqwen.sh):


æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªshellè„šæœ¬ç”¨äºæ¨ç†è¯„æµ‹ä»»åŠ¡ï¼Œå³[src/evaluation/launch_generate_codeqwen.sh](src/evaluation/launch_generate_codeqwen.sh):

```shell
N_NODE=1
N_GPU_PER_NODE=1


tasks=(humanevalsynthesize-python humanevalsynthesize-java humanevalsynthesize-js humanevalsynthesize-cpp humanevalsynthesize-go humanevalsynthesize-rust humanevalfixtests-python humanevalfixtests-java humanevalfixtests-js humanevalfixtests-cpp humanevalfixtests-go humanevalfixtests-rust mbpp)


model=/path/to/local/model/checkpoint
model_name={your-model-name}
generation_base_dir=/path/to/hold/generated/results


if [ ! -d $generation_base_dir ]; then
    mkdir $generation_base_dir
fi


batch_size=1
n_samples=1
# For qwen base model, eos is '<|endoftext|>'; for fine-tuned qwen model, eos is '<|im_end|>'
eos_token="<|im_end|>"


# SFT Format
user=user
assistant=assistant
system=system
end_tag="<|im_end|>"
start_tag="<|im_start|>"

# If you need to set system prompt, set it here, otherwise you can set it as empty string. Decide whether to add system prompt by yourself.
system="$start_tag"$system$'\n'"$end_tag"$'\n'

for task in "${tasks[@]}"; do

    if [ "$task" == "mbpp" ]; then
        prefix="$system""$start_tag"${user}$'\n'
        suffix="$end_tag"$'\n'"$start_tag"${assistant}
    else
        prefix=""
        suffix=""
    fi

    generations_path=$generation_base_dir/generations_$model_name/generations_$task\_$model_name.json
    if [ ! -d $generation_base_dir/generations_$model_name ]; then
        mkdir $generation_base_dir/generations_$model_name
    fi

    echo "start to launch ...."
    accelerate launch \
            --num_machines $N_NODE \
            --num_processes $(($N_NODE*$N_GPU_PER_NODE)) \
            main.py \
                --model $model \
                --task $task \
                --prompt instruct \
                --n_samples $n_samples \
                --batch_size $batch_size \
                --max_length_generation 2000 \
                --do_sample False \
                --temperature 0.2 \
                --precision bf16 \
                --eos "$eos_token" \
                --seed 999999999 \
                --add_special_tokens True \
                --trust_remote_code \
                --generation_only \
                --save_generations_path $generations_path \
                --prefix "$prefix" \
                --suffix "$suffix"
    
    echo "Task $task done"
done
```

To run this script, you must provide the values of local model path ```model```, model name ```model_name```, generated results' base directory ```generation_base_dir```.

æ¬²è¿è¡Œè¯¥è„šæœ¬ï¼Œå¿…é¡»æä¾›æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå³```model```çš„å€¼ï¼‰ã€æ¨¡å‹åå­—ï¼ˆå³```model_name```çš„å€¼ï¼‰ã€ç”Ÿæˆç»“æœå­˜æ”¾ç›®å½•ï¼ˆå³```generation_base_dir```çš„å€¼ï¼‰ã€‚

You have the flexibility to modify other parameters according to your specific requirements. For instance, if you wish to customize the system prompt, you can change the value of the current ```system``` variable. Similarly, if you intend to infer humanevalpack-tasks in the fine-tuned format, which necessitates your model's ability to complete tasks in the fine-tuned format, you will need to adjust the ```prefix``` and ```suffix``` variables when executing humanevalpack tasks.

ä½ å¯ä»¥æŒ‰éœ€çµæ´»è°ƒæ•´å…¶ä»–å‚æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¦ä½¿ç”¨å®šåˆ¶çš„systemæç¤ºï¼Œåˆ™å¯è°ƒæ•´å½“å‰```system```å˜é‡çš„å€¼ï¼›å¦‚æœæƒ³ç”¨å¾®è°ƒæ ¼å¼æ¨ç†HumanEvalPackå„ä»»åŠ¡çš„ç»“æœï¼Œåˆ™å¯è°ƒæ•´æ¼”ç®—è¿™äº›ä»»åŠ¡æ—¶```prefix```å’Œ```suffix```çš„å€¼ï¼Œå½“å‰å‰ææ˜¯æ¨¡å‹èƒ½æ”¯æŒä»¥è¿™ç§æ ¼å¼æ¨ç†è¡¥å…¨ä»»åŠ¡ã€‚

Besides, current script is not task-parallel, you can change it with Slurm as your needs.

é™¤æ­¤ä¹‹å¤–ï¼Œå½“å‰è„šæœ¬ä¸æ˜¯å„å­ä»»åŠ¡å¹¶è¡Œçš„ï¼Œå¯æŒ‰éœ€è‡ªè¡Œè°ƒæ•´ä¸ºå¹¶è¡Œç‰ˆæœ¬ã€‚

Upon running the script for inference, you will obtain a folder named **generations_{your-model-name}**. Within this folder, you will find *13* JSON files named according to the schema **generations_{task-name}_{your-model-name}.json**. Please remember to replace "{your-model-name}" with your specific model name, such as "generations_qwen_1_8B_codefuse".

æ¨ç†å®Œæˆåï¼Œä½ å°†å¾—åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæ–‡ä»¶å¤¹å‘½åä¸º**generations_{your-model-name}**ï¼Œåœ¨å…¶ä¸­æœ‰*13*ä¸ªJSONæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶å‡æŒ‰**generations_{task-name}_{your-model-name}.json**æ¨¡å¼å‘½åã€‚è¿™é‡Œï¼Œ{your-model-name}éœ€æ›¿æ¢ä¸ºä½ å…·ä½“çš„æ¨¡å‹åå­—ï¼Œä¾‹å¦‚"generations_qwen_1_8B_codefuse"ï¼Œ{task-name}éœ€æ›¿æ¢ä¸ºå…·ä½“çš„è¯„æµ‹ä»»åŠ¡åå­—ï¼Œä¾‹å¦‚â€œhumanevalsyntheize-pythonâ€ã€‚


```
generations_{your-model-name}:
\
  - generations_humanevalsynthesize-python_{your-model-name}.json
  - generations_humanevalsynthesize-java_{your-model-name}.json
  - generations_humanevalsynthesize-js_{your-model-name}.json
  - generations_humanevalsynthesize-cpp_{your-model-name}.json
  - generations_humanevalsynthesize-go_{your-model-name}.json
  - generations_humanevalsynthesize-rust_{your-model-name}.json
  - generations_humanevalfixtests-python_{your-model-name}.json
  - generations_humanevalfixtests-java_{your-model-name}.json
  - generations_humanevalfixtests-js_{your-model-name}.json
  - generations_humanevalfixtests-cpp_{your-model-name}.json
  - generations_humanevalfixtests-go_{your-model-name}.json
  - generations_humanevalfixtests-rust_{your-model-name}.json
  - generations_mbpp_{your-model-name}.json
```
**You must not change these names, otherwise your submission will be 0 score. Besides, we require you must generate reference results in  greedy decoding mode, i.e. 
```doSample=Fase, num_beams=1, num_return_sequences=1```**

**ç»ä¸è¦éšæ„æ›´æ”¹è¿™äº›åå­—ï¼Œä»¥å…æˆç»©è¢«åˆ¤å®šä¸º0åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¦æ±‚ç»“æœç”Ÿæˆå¿…é¡»ä½¿ç”¨è´ªå¿ƒè§£ç æ¨¡å¼ï¼Œå³```doSample=Fase, num_beams=1, num_return_sequences=1```**

# Evaluation (Optional) / æµ‹è¯•ï¼ˆå¯é€‰ï¼‰

Due to security concerns, the generated code is executed within a separate container. To facilitate this process and obtain the PASS@1 evaluation scores, we have provided a Docker image and a shell script specifically for running the generated code.
**In the Qwen AI (Code) competition, we adopt "Greedy Decoding Mode & PASS@1" as the metric for evaluation.**

å‡ºäºå®‰å…¨è€ƒè™‘ï¼Œæ¨ç†ç”Ÿæˆçš„ä»£ç å°†åœ¨ä¸€ä¸ªç‹¬ç«‹çš„å®¹å™¨ä¸­è¿è¡Œã€‚ä¸ºæ–¹ä¾¿è‡ªè¡Œè¯„æµ‹å¾—å‡ºPASS@1åˆ†å€¼ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç°æˆå¯ç”¨çš„Dockeré•œåƒå’Œç›¸åº”çš„shellå¤„ç†è„šæœ¬ã€‚
**åœ¨Qwen AIæ¯”èµ›ä¸­ï¼Œæˆ‘ä»¬è¦æ±‚ä½¿ç”¨è´ªå¿ƒè§£ç æ¨¡å¼å¹¶å°†PASS@1ä½œä¸ºè¯„æµ‹æŒ‡æ ‡ã€‚**


## Docker image / Dockeré•œåƒ

You can pull our built image with the following commands:

ä½ å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ‹‰å–é•œåƒï¼š

```bash
docker pull registry.cn-hangzhou.aliyuncs.com/bingchang/code-qwen-competition:latest
docker tag registry.cn-hangzhou.aliyuncs.com/bingchang/code-qwen-competition:latest code-qwen-competition:latest 
```

Also, you can build by yourself with our provide [Dockerfile](src/evaluation/Dockerfile).

ä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„[Dockerfile](src/evaluation/Dockerfile)è‡ªå·±æ„å»ºé•œåƒã€‚

## Running Scripts / è¿è¡Œè„šæœ¬

We provide a shell script to perform evaluation with our provided image in [src/evaluation//launch_evaluate.sh](src/evaluation//launch_evaluate.sh):

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªshellè„šæœ¬[src/evaluation//launch_evaluate.sh](src/evaluation//launch_evaluate.sh)ç”¨äºå®Œæˆä»»åŠ¡pass@1è¯„æµ‹ï¼š


```shell

# please replace this with your own model name which is taken during generation with launch_generate_codeqwen.sh
model={your-model-name}
org=test


tasks=(humanevalsynthesize-python humanevalsynthesize-java humanevalsynthesize-js humanevalsynthesize-cpp humanevalsynthesize-go humanevalsynthesize-rust humanevalfixtests-python humanevalfixtests-java humanevalfixtests-js humanevalfixtests-cpp humanevalfixtests-go humanevalfixtests-rust mbpp)

# if you provide absolute paths remove the $(pwd) from the command below
generations_path=generations_$model
metrics_path=metrics_$model

if [ -d $metrics_path ]; then
    echo "Folder exists. Deleting folder: $metrics_path"
    rm -rf $metrics_path
fi
mkdir $metrics_path

batch_size=1
n_samples=1
eos_token="\"<|im_end|>\""


for task in "${tasks[@]}"; do
    echo "Task: $task"

    gen_suffix=generations_$task\_$model.json
    metric_suffix=metrics_$task\_$model.json
    echo "Evaluation of $model on $task benchmark, data in $generations_path/$gen_suffix"

    sudo docker run -v $(pwd)/$generations_path/$gen_suffix:/app/$gen_suffix:ro  -v $(pwd)/$metrics_path:/app/$metrics_path -it code-qwen-competition bash -c "python3 main.py \
        --model $org/$model \
        --tasks $task \
        --load_generations_path /app/$gen_suffix \
        --metric_output_path /app/$metrics_path/$metric_suffix \
        --allow_code_execution  \
        --trust_remote_code \
        --use_auth_token \
        --temperature 0.2 \
        --max_length_generation 1024 \
        --do_sample False \
        --precision bf16 \
        --eos "$eos_token" \
        --seed 999999999 \
        --batch_size $batch_size \
        --n_samples $n_samples | tee -a logs_$model.txt"
    echo "Task $task done, metric saved at $metrics_path/$metric_suffix"
done
```

In order to execute this script, you must provide the ```{your-model-name}``` parameter and ensure that the generations folder (named *generations_{your-model-name}*) is placed in the same directory as this script. If the paths differ, you will need to adjust the ```${pwd}/generations_path/``` section accordingly.
*Please note that this script does not support task parallelism, but you can modify it according to your specific requirements.*

ä¸ºæ‰§è¡Œè¯¥è„šæœ¬ï¼Œä½ å¿…é¡»æä¾›```{your-model-name}```å‚æ•°çš„å€¼å¹¶ç¡®ä¿ä¿å­˜ç”Ÿæˆç»“æœçš„æ–‡ä»¶å¤¹ï¼ˆå‘½åæ¨¡å¼ä¸º*generations_{your-model-name}*ï¼‰ä¸è¯¥è„šæœ¬æ”¾ç½®åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹ï¼Œå¦‚æœä¸åŒï¼Œåˆ™éœ€å¯¹åº”çš„è°ƒæ•´è„šæœ¬ä¸­çš„```${pwd}/generations_path/```éƒ¨åˆ†ã€‚

After the evaluation process, you will find a metric folder named **metrices_{your-model-name}**, which contains 13 JSON files. Each JSON file corresponds to an evaluation task and holds the evaluation score for that particular task.

è¯„æµ‹å®Œæˆåï¼Œä½ å°†è·å¾—ä¸€ä¸ªå‘½åä¸º**metrices_{your-model-name}**çš„æ–‡ä»¶å¤¹ï¼Œå®ƒé‡Œé¢åŒ…å«13ä¸ªJSONæ–‡ä»¶ï¼Œæ¯ä¸ªJSONæ–‡ä»¶å­˜å‚¨ä¸€ä¸ªè¯„æµ‹ä»»åŠ¡çš„å¾—åˆ†ã€‚

# Submission / æäº¤

Once you have obtained the generated results of your model, which is the folder named "generations_{your-model-name}", you should compress it into a zip file. After compressing the folder, you can proceed to upload the zip file to TianChi platform of Aliyun [https://tianchi.aliyun.com/competition/entrance/532169](https://tianchi.aliyun.com/competition/entrance/532169).


å½“ä½ è·å¾—æ¨¡å‹æ¨ç†ç»“æœåï¼ˆå³è·å¾—ä¸€ä¸ªåä¸º"generations_{your-model-name}"ï¼‰çš„æ–‡ä»¶å¤¹ï¼Œä½ éœ€å°†è¯¥æ–‡ä»¶å¤¹å‹ç¼©ä¸ºä¸€ä¸ªZIPæ–‡ä»¶ã€‚éšåï¼Œä½ éœ€è¦å°†è¯¥ZIPæ–‡ä»¶ä¸Šä¼ åˆ°é˜¿é‡Œäº‘å¤©æ± å¹³å°[https://tianchi.aliyun.com/competition/entrance/532169](https://tianchi.aliyun.com/competition/entrance/532169)å®Œæˆæ‰“åˆ†ã€‚


Your submission must satisfy these requirements:

ä½ çš„æäº¤å¿…é¡»æ»¡è¶³å¦‚ä¸‹è¦æ±‚ï¼š

```text
1. The generation result folder must be compressed into a zip file
2. The decompressed result of the zip file must be a folder named with "generations_{your-model-name}"
3. Verify that the folder contains exactly 13 JSON files, each corresponding to one evaluation task.
4. Name each JSON file using the following schema: "generations_{task-name}_{your-model-name}.json". Replace "{task-name}" with the name of the evaluation task and "{your-model-name}" with the name of your model.
```

```text
1. ç”Ÿæˆç»“æœæ–‡ä»¶å¤¹éœ€å‹ç¼©è¿›ä¸€ä¸ªZIPæ–‡ä»¶
2. ZIPæ–‡ä»¶éœ€èƒ½è§£å‹å‡ºä¸€ä¸ªå‘½åæ¨¡å¼ä¸º"generations_{your-model-name}"çš„æ–‡ä»¶å¤¹
3. è§£å‹å‡ºçš„æ–‡ä»¶å¤¹ä¸­éœ€åˆšå¥½åŒ…å«13ä¸ªJSONæ–‡ä»¶ï¼Œæ¯ä¸ªJSONæ–‡ä»¶å¯¹åº”äºä¸€ä¸ªè¯„æµ‹ä»»åŠ¡
3. æ¯ä¸ªJSONæ–‡ä»¶æŒ‰"generations_{task-name}_{your-model-name}.json"æ¨¡å¼å‘½åï¼Œå…¶ä¸­ï¼Œ"{task-name}"æ˜¯è¯„æµ‹ä»»åŠ¡çš„åå­—ï¼Œ"{your-model-name}"æ˜¯æ¨ç†æ—¶è®¾ç½®çš„æ¨¡å‹åå­—
```

Once you have submitted your generated results, the TianChi platform will evaluate the PASS@1 scores. The average score across all 13 tasks will then be calculated and considered as your overall score for the submission.

å½“ä½ æäº¤ç”Ÿæˆç»“æœåï¼Œå¤©æ± å¹³å°ä¼šè¯„æµ‹å¾—å‡ºPASS@1åˆ†å€¼ï¼Œå¹¶å°†13ä¸ªä»»åŠ¡çš„å¹³å‡PASS@1å€¼ä½œä¸ºæœ¬æ¬¡æäº¤çš„å¾—åˆ†ã€‚

ä¸‹é¢æ˜¯æˆ‘ä»¬æœªç²¾ç»†å¾®è°ƒçš„ä¸€ä¸ªç‰ˆæœ¬åˆ†æ•°ï¼Œä½œä¸ºæœ¬æ¬¡å¤§èµ›çš„ baseline åˆ†æ•°:
```
score:0.0933
humanevalsynthesize-python:0.2195
humanevalfixtests-cpp:0.0183
mbpp:0.2440
humanevalsynthesize-cpp:0.1280
humanevalfixtests-js:0.0122
humanevalfixtests-go:0.0122
humanevalfixtests-python:0.0305
humanevalfixtests-rust:0.0000
humanevalsynthesize-rust:0.0366
humanevalsynthesize-js:0.1341
humanevalsynthesize-go:0.1524
humanevalfixtests-java:0.0000
humanevalsynthesize-java:0.2256
```
